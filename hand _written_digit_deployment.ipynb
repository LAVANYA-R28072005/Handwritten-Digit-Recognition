{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb16d28-e31c-4965-a7ad-1ec49ed263db",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-success\" dir=\"rtl\" style=\"text-align: center;\"><strong><span style=\"font-size: 20pt\">Hand written digit recognition <br /></span></strong></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc68df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9c9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to values between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reshape the data to include a single color channel\n",
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba2c39",
   "metadata": {},
   "source": [
    "# Brif info about Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32658ff7",
   "metadata": {},
   "source": [
    "The MNIST (Modified National Institute of Standards and Technology) dataset is a widely used benchmark in machine learning, particularly for training and testing models in image classification tasks. It consists of 70,000 grayscale images of handwritten digits, ranging from 0 to 9. \n",
    "\n",
    "- **Training Set**: 60,000 images\n",
    "- **Test Set**: 10,000 images\n",
    "- **Image Size**: Each image is 28x28 pixels, resulting in 784 pixels per image.\n",
    "- **Label**: Each image is labeled with the corresponding digit (0-9) it represents.\n",
    "\n",
    "The dataset is often used to demonstrate the performance of various machine learning algorithms, particularly in the context of neural networks and deep learning. Its simplicity and standardized format make it an excellent starting point for beginners in the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0e493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\my_program_files\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#BUILD THE MODEL\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe95d5",
   "metadata": {},
   "source": [
    "# brif about building model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c438287",
   "metadata": {},
   "source": [
    "1. Model Initialization\n",
    "models.Sequential(): This initializes a sequential model, which means that the layers will be stacked one after another in a linear fashion.\n",
    "2. First Convolutional Layer\n",
    "Conv2D(32, (3, 3), activation='relu'): This is a 2D convolutional layer that applies 32 filters, each of size 3x3, to the input image. The relu (Rectified Linear Unit) activation function introduces non-linearity, helping the model to learn complex patterns.\n",
    "input_shape=(28, 28, 1): This specifies the shape of the input data. The images are 28x28 pixels with 1 channel (grayscale).\n",
    "3. First Pooling Layer\n",
    "MaxPooling2D((2, 2)): This is a max pooling layer with a 2x2 filter, which reduces the spatial dimensions (height and width) of the feature maps by taking the maximum value from each 2x2 block. This helps in reducing the computational complexity and overfitting.\n",
    "4. Second Convolutional Layer\n",
    "Conv2D(64, (3, 3), activation='relu'): Another convolutional layer, but this time with 64 filters of size 3x3. The increased number of filters allows the model to learn more complex features.\n",
    "5. Second Pooling Layer\n",
    "MaxPooling2D((2, 2)): Another max pooling layer to further reduce the spatial dimensions of the feature maps.\n",
    "6. Third Convolutional Layer\n",
    "Conv2D(64, (3, 3), activation='relu'): A third convolutional layer with 64 filters of size 3x3. This deepens the network, allowing it to learn more abstract features.\n",
    "7. Flattening Layer\n",
    "Flatten(): This layer converts the 2D feature maps (output from the previous convolutional layer) into a 1D vector. This vector will be fed into the fully connected (dense) layers.\n",
    "8. First Dense (Fully Connected) Layer\n",
    "Dense(64, activation='relu'): A dense layer with 64 neurons, each connected to all the neurons in the previous layer. The relu activation function is used here to introduce non-linearity.\n",
    "9. Output Layer\n",
    "Dense(10, activation='softmax'): The output layer with 10 neurons, one for each class (0-9) in the MNIST dataset. The softmax activation function is used to convert the output into probabilities, where the highest probability corresponds to the predicted digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3a1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILE THE MODEL\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.8979 - loss: 0.3322 - val_accuracy: 0.9860 - val_loss: 0.0433\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9842 - loss: 0.0503 - val_accuracy: 0.9885 - val_loss: 0.0342\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9891 - loss: 0.0345 - val_accuracy: 0.9881 - val_loss: 0.0375\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9916 - loss: 0.0243 - val_accuracy: 0.9904 - val_loss: 0.0293\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - accuracy: 0.9938 - loss: 0.0175 - val_accuracy: 0.9915 - val_loss: 0.0290\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9956 - loss: 0.0141 - val_accuracy: 0.9905 - val_loss: 0.0329\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0116 - val_accuracy: 0.9911 - val_loss: 0.0310\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0117 - val_accuracy: 0.9910 - val_loss: 0.0331\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.9971 - loss: 0.0083 - val_accuracy: 0.9915 - val_loss: 0.0313\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9974 - loss: 0.0084 - val_accuracy: 0.9910 - val_loss: 0.0363\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9975 - loss: 0.0073 - val_accuracy: 0.9900 - val_loss: 0.0388\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 0.9926 - val_loss: 0.0369\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 0.9895 - val_loss: 0.0452\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9976 - loss: 0.0061 - val_accuracy: 0.9927 - val_loss: 0.0332\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9983 - loss: 0.0063 - val_accuracy: 0.9919 - val_loss: 0.0367\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 0.9925 - val_loss: 0.0383\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - accuracy: 0.9981 - loss: 0.0050 - val_accuracy: 0.9920 - val_loss: 0.0363\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0058 - val_accuracy: 0.9913 - val_loss: 0.0474\n",
      "Epoch 19/20\n",
      "\u001b[1m 932/1875\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0036"
     ]
    }
   ],
   "source": [
    "#TRAIN THE MODEL\n",
    "model.fit(train_images, train_labels, epochs=20, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE THE MODEL PERFORMANCE\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d105f5",
   "metadata": {},
   "source": [
    "WE CAN SEE THE RESULT IS VERY PROMISSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Displaying the first image, its predicted label, and the true label\n",
    "plt.imshow(test_images[0].reshape(28, 28), cmap=plt.cm.binary)\n",
    "plt.title(f\"Predicted: {np.argmax(predictions[0])}, True: {test_labels[0]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30597749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK OTHER IMAGES\n",
    "\n",
    "index = 10  # Change this to check other images (0-based index)\n",
    "plt.imshow(test_images[index].reshape(28, 28), cmap=plt.cm.binary)\n",
    "plt.title(f\"Predicted: {np.argmax(predictions[index])}, True: {test_labels[index]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):  # Change 10 to the number of images you want to check\n",
    "    plt.imshow(test_images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.title(f\"Predicted: {np.argmax(predictions[i])}, True: {test_labels[i]}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04c9a0",
   "metadata": {},
   "source": [
    "WE CHECK WITH A QUICK OVERVIEW OF MODEL'S PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7af290-5b34-49cf-89f0-c9afc1be9fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cac821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def show_prediction(index):\n",
    "    plt.imshow(test_images[index].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.title(f\"Predicted: {np.argmax(predictions[index])}, True: {test_labels[index]}\")\n",
    "    plt.show()\n",
    "\n",
    "# Slider to choose the image index\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_images) - 1, step=1)\n",
    "widgets.interactive(show_prediction, index=index_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773433",
   "metadata": {},
   "source": [
    "# HERE WE CREATE A SLIDE BAR FOR CHECKING THE MODELS PERFORMANCE AND WE CAN CHECK THE MODEL IS PREDICTING EVERY SINGLE DIGIT CORRECTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10  # Number of images to display\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.imshow(test_images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.title(f\"P: {np.argmax(predictions[i])}\\nT: {test_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96069f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_digit_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8492e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46cbab",
   "metadata": {},
   "source": [
    "# BRIF ABOUT PILLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188a497",
   "metadata": {},
   "source": [
    "Pillow is a Python Imaging Library (PIL) fork that adds image processing capabilities to Python. In the context of deep learning, Pillow is often used for:\n",
    "\n",
    "1. **Image Preprocessing**: Pillow can be used to open, manipulate, and save image files. Common tasks include resizing, cropping, rotating, and converting images between different formats (e.g., JPEG, PNG).\n",
    "\n",
    "2. **Data Augmentation**: Pillow can perform simple data augmentation techniques like flipping, rotating, or adjusting brightness, which helps in creating variations of the training data, making models more robust.\n",
    "\n",
    "3. **Integration with Deep Learning Libraries**: Pillow can easily be used with popular deep learning frameworks like TensorFlow and PyTorch for preprocessing image data before feeding it into a model.\n",
    "\n",
    "Overall, Pillow is a handy tool in the deep learning workflow for preparing and augmenting image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE  A SIMPLE GUI WITH CANVAS\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('mnist_digit_recognition_model.h5')\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Digit Recognizer\")\n",
    "\n",
    "# Create a canvas for drawing\n",
    "canvas = tk.Canvas(window, width=200, height=200, bg='white')\n",
    "canvas.grid(row=0, column=0, pady=2, sticky=tk.W, columnspan=2)\n",
    "\n",
    "# Set up PIL to draw on the canvas\n",
    "image = Image.new(\"L\", (200, 200), color=255)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Function to draw on the canvas\n",
    "def paint(event):\n",
    "    x1, y1 = (event.x - 8), (event.y - 8)\n",
    "    x2, y2 = (event.x + 8), (event.y + 8)\n",
    "    canvas.create_oval(x1, y1, x2, y2, fill='black', width=15)\n",
    "    draw.line([x1, y1, x2, y2], fill='black', width=15)\n",
    "\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# Function to make a prediction\n",
    "def predict_digit():\n",
    "    # Resize and invert the image\n",
    "    img = image.resize((28, 28))\n",
    "    img = np.array(img)\n",
    "    img = 255 - img  # Invert image colors\n",
    "    img = img / 255.0  # Normalize the image\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "\n",
    "    # Predict the digit\n",
    "    prediction = model.predict([img])\n",
    "    predicted_digit = np.argmax(prediction)\n",
    "    \n",
    "    # Display the result\n",
    "    result_label.config(text=f\"Predicted Digit: {predicted_digit}\")\n",
    "\n",
    "# Function to clear the canvas\n",
    "def clear_canvas():\n",
    "    canvas.delete(\"all\")\n",
    "    draw.rectangle([0, 0, 200, 200], fill=\"white\")\n",
    "\n",
    "# Add prediction and clear buttons\n",
    "predict_button = tk.Button(window, text=\"Predict\", command=predict_digit)\n",
    "predict_button.grid(row=1, column=0, pady=2)\n",
    "clear_button = tk.Button(window, text=\"Clear\", command=clear_canvas)\n",
    "clear_button.grid(row=1, column=1, pady=2)\n",
    "\n",
    "# Add a label to show the prediction result\n",
    "result_label = tk.Label(window, text=\"Draw a digit and press Predict\")\n",
    "result_label.grid(row=2, column=0, columnspan=2)\n",
    "\n",
    "# Run the tkinter loop\n",
    "window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
